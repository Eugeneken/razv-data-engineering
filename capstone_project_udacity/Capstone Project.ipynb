{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline: Anime Recommendation Database\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project:\n",
    "\n",
    "\n",
    "* Download Anime Recommendation Database 2020 data from Kaggle and load raw dataset to AWS S3.\n",
    "* Create an ETL pipeline that extracts, processes using Spark, and loads the data back into set of dimensional tables in S3.\n",
    "\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config_object = configparser.ConfigParser()\n",
    "config_object.read_file(open('dl.cfg'))\n",
    "profile_info = config_object[\"aws_profile\"]\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", profile_info.get('aws_key_id')) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", profile_info.get('aws_secret_access_key')) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will perform transformations on two datasets. We will then create two dimension tables, each for the respective dataset. The resulting databases are intended to be used as analytical foundation, and data for creating a recommendation system . Spark will be used to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Source data:\n",
    "\n",
    "https://www.kaggle.com/hernan4444/anime-recommendation-database-2020?select=anime.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### About Files\n",
    "\n",
    "* `animelist.csv`: List of all animes register by the user with the respective score, watching status and numbers of episodes watched. This dataset contains 109 Million row, 17.562 different animes and 325.772 different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- anime_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- watching_status: string (nullable = true)\n",
      " |-- watched_episodes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_list_path = \"s3a://capstone-final-project-udacity/animelist.csv\"\n",
    "anime_list_df = spark.read.csv(anime_list_path, header=True)\n",
    "anime_list_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------------+----------------+\n",
      "|user_id|anime_id|rating|watching_status|watched_episodes|\n",
      "+-------+--------+------+---------------+----------------+\n",
      "|0      |67      |9     |1              |1               |\n",
      "|0      |6702    |7     |1              |4               |\n",
      "|0      |242     |10    |1              |4               |\n",
      "|0      |4898    |0     |1              |1               |\n",
      "|0      |21      |10    |1              |0               |\n",
      "+-------+--------+------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_list_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* `anime.csv`: Information of anime scrapped of main page and stats page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MAL_ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Genres: string (nullable = true)\n",
      " |-- English name: string (nullable = true)\n",
      " |-- Japanese name: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Episodes: string (nullable = true)\n",
      " |-- Aired: string (nullable = true)\n",
      " |-- Premiered: string (nullable = true)\n",
      " |-- Producers: string (nullable = true)\n",
      " |-- Licensors: string (nullable = true)\n",
      " |-- Studios: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- Ranked: string (nullable = true)\n",
      " |-- Popularity: string (nullable = true)\n",
      " |-- Members: string (nullable = true)\n",
      " |-- Favorites: string (nullable = true)\n",
      " |-- Watching: string (nullable = true)\n",
      " |-- Completed: string (nullable = true)\n",
      " |-- On-Hold: string (nullable = true)\n",
      " |-- Dropped: string (nullable = true)\n",
      " |-- Plan to Watch: string (nullable = true)\n",
      " |-- Score-10: string (nullable = true)\n",
      " |-- Score-9: string (nullable = true)\n",
      " |-- Score-8: string (nullable = true)\n",
      " |-- Score-7: string (nullable = true)\n",
      " |-- Score-6: string (nullable = true)\n",
      " |-- Score-5: string (nullable = true)\n",
      " |-- Score-4: string (nullable = true)\n",
      " |-- Score-3: string (nullable = true)\n",
      " |-- Score-2: string (nullable = true)\n",
      " |-- Score-1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_path = \"s3a://capstone-final-project-udacity/anime.csv\"\n",
    "anime_df = spark.read.csv(anime_path, header=True)\n",
    "anime_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Removing Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MAL_ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Genres: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Episodes: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Ranked: string (nullable = true)\n",
      " |-- Popularity: string (nullable = true)\n",
      " |-- Members: string (nullable = true)\n",
      " |-- Favorites: string (nullable = true)\n",
      " |-- Watching: string (nullable = true)\n",
      " |-- Completed: string (nullable = true)\n",
      " |-- On-Hold: string (nullable = true)\n",
      " |-- Dropped: string (nullable = true)\n",
      " |-- Plan to Watch: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_df = anime_df.drop(\"Japanese name\", \"English name\", \"Score-1\", \"Score-2\", \"Score-3\", \"Score-4\", \"Score-5\", \"Score-6\", \"Score-7\", \"Score-8\", \"Score-9\",\n",
    "                         \"Score-10\", \"Rating\", \"Premiered\", \"Aired\", \"Producers\", \"Studios\", \"Duration\", \"Licensors\")\n",
    "anime_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Convert column genres to column array type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_df=anime_df.select(\"MAL_ID\", \"Name\", \"Score\", \"Type\", \"Episodes\", \"Source\", \"Ranked\",\n",
    "             \"Popularity\", \"Members\", \"Favorites\", \"Watching\", \"Completed\",\n",
    "             \"On-Hold\", \"Dropped\", \"Plan to Watch\", f.split(\"Genres\", \",\").alias(\"genre\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Drop adult animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_df=anime_df.select(\"MAL_ID\", \"Name\", \"Score\", \"Type\", \"Episodes\", \"Source\", \"Ranked\",\n",
    "                         \"Popularity\", \"Members\", \"Favorites\", \"Watching\",\"Completed\",\n",
    "                         \"On-Hold\", \"Dropped\", \"Plan to Watch\", \"genre\", f.array_contains(anime_df.genre, \"Hentai\").alias(\"check\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+-----+--------+--------+------+----------+-------+---------+--------+---------+-------+-------+-------------+--------------------+\n",
      "|MAL_ID|                Name|Score| Type|Episodes|  Source|Ranked|Popularity|Members|Favorites|Watching|Completed|On-Hold|Dropped|Plan to Watch|               genre|\n",
      "+------+--------------------+-----+-----+--------+--------+------+----------+-------+---------+--------+---------+-------+-------+-------------+--------------------+\n",
      "|     1|        Cowboy Bebop| 8.78|   TV|      26|Original|  28.0|        39|1251960|    61971|  105808|   718161|  71513|  26678|       329800|[Action,  Adventu...|\n",
      "|     5|Cowboy Bebop: Ten...| 8.39|Movie|       1|Original| 159.0|       518| 273145|     1174|    4143|   208333|   1935|    770|        57964|[Action,  Drama, ...|\n",
      "|     6|              Trigun| 8.24|   TV|      26|   Manga| 266.0|       201| 558913|    12944|   29113|   343492|  25465|  13925|       146918|[Action,  Sci-Fi,...|\n",
      "|     7|  Witch Hunter Robin| 7.27|   TV|      26|Original|2481.0|      1467|  94683|      587|    4300|    46165|   5121|   5378|        33719|[Action,  Mystery...|\n",
      "|     8|      Bouken Ou Beet| 6.98|   TV|      52|   Manga|3710.0|      4369|  13224|       18|     642|     7314|    766|   1108|         3394|[Adventure,  Fant...|\n",
      "+------+--------------------+-----+-----+--------+--------+------+----------+-------+---------+--------+---------+-------+-------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_df=(anime_df.select(\"MAL_ID\", \"Name\", \"Score\", \"Type\", \"Episodes\", \"Source\", \"Ranked\",\n",
    "                          \"Popularity\", \"Members\", \"Favorites\", \"Watching\", \"Completed\", \n",
    "                          \"On-Hold\", \"Dropped\", \"Plan to Watch\", \"genre\").where(anime_df.check == \"False\"))\n",
    "anime_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Generating unique ID's for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_list_df = anime_list_df.drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_list_df=anime_list_df.select(\"anime_id\", \"rating\", \"watching_status\", \"watched_episodes\").withColumn(\"user_id\", f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------+----------------+-------+\n",
      "|anime_id|rating|watching_status|watched_episodes|user_id|\n",
      "+--------+------+---------------+----------------+-------+\n",
      "|      67|     9|              1|               1|      0|\n",
      "|    6702|     7|              1|               4|      1|\n",
      "|     242|    10|              1|               4|      2|\n",
      "|    4898|     0|              1|               1|      3|\n",
      "|      21|    10|              1|               0|      4|\n",
      "+--------+------+---------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_list_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "This will allow the review team to find information about the anime and the creation of a recommendation system.\n",
    "\n",
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The first dimension table will contain the list of all animes register by the user with the respective score, watching status and numbers of episodes watched.\n",
    "\n",
    "* `user_id`: Randomly generated user id.\n",
    "* `anime_id`: ID of the anime. (e.g. 1).\n",
    "* `rating`: Score between 1 to 10 given by the user. 0 if the user didn't assign a score. (e.g. 10)\n",
    "* `watching_status`: State ID from this anime in the anime list of this user. (e.g. 2)\n",
    "* `watched_episodes`: Numbers of episodes watched by the user. (e.g. 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The second dimension table will contain general information of every anime (17.562 different anime).\n",
    "\n",
    "* `MAL_ID`: MyAnimelist ID of the anime. (e.g. 1)\n",
    "* `Name`: full name of the anime. (e.g. Cowboy Bebop)\n",
    "* `Score`: average score of the anime given from all users in MyAnimelist database. (e.g. 8.78)\n",
    "* `Genre`: comma separated list of genres for this anime. (e.g. Action, Adventure, Comedy, Drama, Sci-Fi, Space)\n",
    "* `Type`: TV, movie, OVA, etc. (e.g. TV)\n",
    "* `Episodes`: number of chapters. (e.g. 26)\n",
    "* `Source`: Manga, Light novel, Book, etc. (e.g Original)\n",
    "* `Ranked`: position based in the score. (e.g 28)\n",
    "* `Popularity`: position based in the the number of users who have added the anime to their list. (e.g 39)\n",
    "* `Members`: number of community members that are in this anime's \"group\". (e.g. 1251960)\n",
    "* `Favorites`: number of users who have the anime as \"favorites\". (e.g. 61,971)\n",
    "* `Watching`: number of users who are watching the anime. (e.g. 105808)\n",
    "* `Completed`: number of users who have complete the anime. (e.g. 718161)\n",
    "* `On-Hold`: number of users who have the anime on Hold. (e.g. 71513)\n",
    "* `Dropped`: number of users who have dropped the anime. (e.g. 26678)\n",
    "* `Plan to Watch`: number of users who plan to watch the anime. (e.g. 329800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "* Create anime list and anime general information dimension table.\n",
    "* Change columns to correct date types.\n",
    "* Write tables to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_by_user_table=anime_list_df.select(\"user_id\", \"anime_id\", \"rating\",\n",
    "                                        \"watching_status\", \"watched_episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = false)\n",
      " |-- anime_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- watching_status: string (nullable = true)\n",
      " |-- watched_episodes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_by_user_table=anime_by_user_table.withColumn(\"rating\",col(\"rating\").cast(IntegerType())) \\\n",
    "    .withColumn(\"watched_episodes\",col(\"watched_episodes\").cast(IntegerType()))\n",
    "anime_by_user_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------------+----------------+\n",
      "|user_id|anime_id|rating|watching_status|watched_episodes|\n",
      "+-------+--------+------+---------------+----------------+\n",
      "|      0|      67|     9|              1|               1|\n",
      "|      1|    6702|     7|              1|               4|\n",
      "|      2|     242|    10|              1|               4|\n",
      "|      3|    4898|     0|              1|               1|\n",
      "|      4|      21|    10|              1|               0|\n",
      "+-------+--------+------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_by_user_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "anime_table=anime_df.select(\"MAL_ID\", \"Name\", \"Score\", \"Type\", \"Episodes\", \"Source\", \"Ranked\",\n",
    "                          \"Popularity\", \"Members\", \"Favorites\", \"Watching\", \"Completed\", \n",
    "                          \"On-Hold\", \"Dropped\", \"Plan to Watch\", \"genre\") \\\n",
    ".withColumn(\"score\", col(\"Score\").cast(FloatType())) \\\n",
    ".withColumn(\"episodes\", col(\"Episodes\").cast(IntegerType())) \\\n",
    ".withColumn(\"members\", col(\"Members\").cast(IntegerType())) \\\n",
    ".withColumn(\"favorites\", col(\"Favorites\").cast(IntegerType())) \\\n",
    ".withColumn(\"watching\", col(\"Watching\").cast(IntegerType())) \\\n",
    ".withColumn(\"completed\", col(\"Completed\").cast(IntegerType())) \\\n",
    ".withColumn(\"on_hold\", col(\"On-Hold\").cast(IntegerType())) \\\n",
    ".withColumn(\"dropped\", col(\"Dropped\").cast(IntegerType())) \\\n",
    ".withColumn(\"plan_to_watch\", col(\"Plan to Watch\").cast(IntegerType())) \\\n",
    ".withColumnRenamed(\"MAL_ID\", \"anime_id\") \\\n",
    ".withColumnRenamed(\"Name\", \"name_of_anime\") \\\n",
    ".withColumnRenamed(\"Type\", \"type_of_anime\") \\\n",
    ".withColumnRenamed(\"Source\", \"source\") \\\n",
    ".withColumnRenamed(\"Ranked\", \"ranked\") \\\n",
    ".withColumnRenamed(\"Popularity\", \"popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anime_id: string (nullable = true)\n",
      " |-- name_of_anime: string (nullable = true)\n",
      " |-- score: float (nullable = true)\n",
      " |-- type_of_anime: string (nullable = true)\n",
      " |-- episodes: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- ranked: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- members: integer (nullable = true)\n",
      " |-- favorites: integer (nullable = true)\n",
      " |-- watching: integer (nullable = true)\n",
      " |-- completed: integer (nullable = true)\n",
      " |-- dropped: integer (nullable = true)\n",
      " |-- genre: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- on_hold: integer (nullable = true)\n",
      " |-- plan_to_watch: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_table=anime_table.drop(\"On-Hold\", \"Plan to Watch\")\n",
    "anime_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+-------------+--------+--------+------+----------+-------+---------+--------+---------+-------+--------------------+-------+-------------+\n",
      "|anime_id|       name_of_anime|score|type_of_anime|episodes|  source|ranked|popularity|members|favorites|watching|completed|dropped|               genre|on_hold|plan_to_watch|\n",
      "+--------+--------------------+-----+-------------+--------+--------+------+----------+-------+---------+--------+---------+-------+--------------------+-------+-------------+\n",
      "|       1|        Cowboy Bebop| 8.78|           TV|      26|Original|  28.0|        39|1251960|    61971|  105808|   718161|  26678|[Action,  Adventu...|  71513|       329800|\n",
      "|       5|Cowboy Bebop: Ten...| 8.39|        Movie|       1|Original| 159.0|       518| 273145|     1174|    4143|   208333|    770|[Action,  Drama, ...|   1935|        57964|\n",
      "|       6|              Trigun| 8.24|           TV|      26|   Manga| 266.0|       201| 558913|    12944|   29113|   343492|  13925|[Action,  Sci-Fi,...|  25465|       146918|\n",
      "|       7|  Witch Hunter Robin| 7.27|           TV|      26|Original|2481.0|      1467|  94683|      587|    4300|    46165|   5378|[Action,  Mystery...|   5121|        33719|\n",
      "|       8|      Bouken Ou Beet| 6.98|           TV|      52|   Manga|3710.0|      4369|  13224|       18|     642|     7314|   1108|[Adventure,  Fant...|    766|         3394|\n",
      "+--------+--------------------+-----+-------------+--------+--------+------+----------+-------+---------+--------+---------+-------+--------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Write to AWS S3 Bucket in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path s3a://capstone-final-project-udacity/resultsanime_by_user/anime_by_user.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o357.parquet.\n: org.apache.spark.sql.AnalysisException: path s3a://capstone-final-project-udacity/resultsanime_by_user/anime_by_user.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5da61dee8075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3a://capstone-final-project-udacity/results\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0manime_by_user_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"anime_by_user/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"anime_by_user.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0manime_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"anime_list/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"anime_list.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path s3a://capstone-final-project-udacity/resultsanime_by_user/anime_by_user.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "output_data  = \"s3a://capstone-final-project-udacity/results\" \n",
    "\n",
    "anime_by_user_table.write.parquet(output_data + \"anime_by_user/\" + \"anime_by_user.parquet\")\n",
    "anime_table.write.parquet(output_data + \"anime_list/\" + \"anime_list.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data quality check passed\n",
      "data quality check passed\n"
     ]
    }
   ],
   "source": [
    "def table_exists(df):\n",
    "    if df is not None:\n",
    "        return print(\"data quality check passed\")\n",
    "    else:\n",
    "        return print(\"data quality check failed\")\n",
    "    \n",
    "table_exists(anime_table)\n",
    "table_exists(anime_by_user_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for Animes table with 16398 records\n",
      "Data quality check passed for Animes by user table with 109224747 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe\n",
    "    Output: Print outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0\n",
    "\n",
    "# Perform data quality check\n",
    "quality_check(anime_table, \"Animes table\")\n",
    "quality_check(anime_by_user_table, \"Animes by user table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Anime table\n",
    "\n",
    "* `user_id`: Randomly generated user id.\n",
    "* `anime_id`: ID of the anime. (e.g. 1).\n",
    "* `rating`: Score between 1 to 10 given by the user. 0 if the user didn't assign a score. (e.g. 10)\n",
    "* `watching_status`: State ID from this anime in the anime list of this user. (e.g. 2)\n",
    "* `watched_episodes`: Numbers of episodes watched by the user. (e.g. 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Anime by user table\n",
    "\n",
    "* `MAL_ID`: MyAnimelist ID of the anime. (e.g. 1)\n",
    "* `Name`: full name of the anime. (e.g. Cowboy Bebop)\n",
    "* `Score`: average score of the anime given from all users in MyAnimelist database. (e.g. 8.78)\n",
    "* `Genre`: comma separated list of genres for this anime. (e.g. Action, Adventure, Comedy, Drama, Sci-Fi, Space)\n",
    "* `Type`: TV, movie, OVA, etc. (e.g. TV)\n",
    "* `Episodes`: number of chapters. (e.g. 26)\n",
    "* `Source`: Manga, Light novel, Book, etc. (e.g Original)\n",
    "* `Ranked`: position based in the score. (e.g 28)\n",
    "* `Popularity`: position based in the the number of users who have added the anime to their list. (e.g 39)\n",
    "* `Members`: number of community members that are in this anime's \"group\". (e.g. 1251960)\n",
    "* `Favorites`: number of users who have the anime as \"favorites\". (e.g. 61,971)\n",
    "* `Watching`: number of users who are watching the anime. (e.g. 105808)\n",
    "* `Completed`: number of users who have complete the anime. (e.g. 718161)\n",
    "* `On-Hold`: number of users who have the anime on Hold. (e.g. 71513)\n",
    "* `Dropped`: number of users who have dropped the anime. (e.g. 26678)\n",
    "* `Plan to Watch`: number of users who plan to watch the anime. (e.g. 329800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Spark was chosen since it can easily handle large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Propose how often the data should be updated and why.\n",
    "* The data should be updated monthly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write a description of how you would approach the problem differently under the following scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The data was increased by 100x.\n",
    "* Use AWS EMR + S3 if the data was increased by 100x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "* Use Apache Airflow.\n",
    "* DAG retries, or send emails on failures\n",
    "* Daily intervals with quality checks\n",
    "* If checks fail, then send emails to operators, freeze dashboard, look at DAG logs to figure out what went wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The database needed to be accessed by 100+ people.\n",
    "* Store parquet files in AWS S3, give read access to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
